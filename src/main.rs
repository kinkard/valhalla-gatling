use anyhow::{Context, Result};
use clap::{Parser, Subcommand};
use pcap_parser::traits::PcapReaderIterator;
use pcap_parser::*;
use protobuf::Message;
use reqwest::header::{self, HeaderName, HeaderValue};
use reqwest::Method;
use serde::{Deserialize, Serialize};
use std::fs::File;
use std::str::FromStr;
use std::sync::Arc;

mod proto {
    include!(concat!(env!("OUT_DIR"), "/proto/mod.rs"));
}

#[derive(Parser)]
#[command(author, version, about, long_about = None)]
struct Cli {
    #[command(subcommand)]
    command: Commands,
}

#[derive(Subcommand)]
enum Commands {
    /// Extracts `/route` Valhalla requests from the specified .pcap file.
    Extract {
        /// Path to the .pcap file to extract the requests from, usually captured via `tcpdump`.
        tcpdump: String,
        /// Optional output file name. If not provided, the default name is equal to the input file name with the `.playbook` extension.
        #[arg(short, long)]
        output: Option<String>,
    },

    /// Sends the requests to the specified URL and measures the throughput, success rate and latency percentiles.
    Run {
        /// The URL to send the request to.
        /// Example: http://localhost:8002/route
        url: String,
        /// The playbook file in .playbook format, generated by the `extract` command.
        playbook: String,
    },
}

#[derive(Clone, Copy, Debug, Serialize, Deserialize, PartialEq)]
enum HttpMethod {
    GET,
    POST,
}

impl From<HttpMethod> for reqwest::Method {
    fn from(method: HttpMethod) -> Self {
        match method {
            HttpMethod::GET => Method::GET,
            HttpMethod::POST => Method::POST,
        }
    }
}

#[derive(Serialize, Deserialize)]
struct Request {
    /// HTTP request type like `GET` or `POST`
    method: HttpMethod,
    /// HTTP request headers like `Content-Type` or `Accept-Encoding`
    headers: Vec<(String, String)>,
    /// HTTP request body
    body: Vec<u8>,
}

#[derive(Serialize, Deserialize)]
struct Playbook {
    /// List of requests to be sent to the server
    requests: Vec<Request>,
}

impl Playbook {
    fn save(&self, path: &str) -> Result<()> {
        let mut s = flexbuffers::FlexbufferSerializer::new();
        self.serialize(&mut s)
            .context("Failed to serialize requests")?;
        std::fs::write(path, s.view()).context("Failed to write serialized requests")
    }

    fn load(path: &str) -> Result<Self> {
        let data = std::fs::read(path).context("Failed to read playbook file")?;
        let r = flexbuffers::Reader::get_root(data.as_ref()).context("Failed to create reader")?;
        let playbook = Playbook::deserialize(r).context("Failed to deserialize playbook")?;
        Ok(playbook)
    }
}

#[derive(Default)]
struct Metric {
    concurrency: u16,
    throughput: f64,
    success_rate: f64,
    p50: f64,
    p95: f64,
    p99: f64,
}

fn main() {
    let cli = Cli::parse();

    match cli.command {
        Commands::Extract { tcpdump, output } => extract(tcpdump, output),
        Commands::Run { url, playbook } => run(url, playbook),
    }
}

fn extract(tcpdump: String, output: Option<String>) {
    let output = output.unwrap_or_else(|| format!("{}.playbook", tcpdump));
    let requests = parse_tcpdump(tcpdump).expect("Failed to parse tcpdump");
    let playbook = Playbook { requests };
    playbook.save(&output).expect("Failed to save playbook");
}

#[tokio::main]
async fn run(url: String, playbook: String) {
    // Test url against `/status` endpoint to ensure that Valhalla server is running
    let status = reqwest::get(format!("{url}/status")).await;
    assert!(
        status.is_ok_and(|r| r.status().is_success()),
        "HTTP request to '{url}/status' failed",
    );

    let playbook = Playbook::load(&playbook).expect("Failed to load playbook");
    let requests: Arc<[Request]> = Arc::from(playbook.requests);

    let client = reqwest::Client::new();

    // Doing concurrent requests via spawning tasks allows us to
    // - gradually increase the number of concurrent requests without pauses
    // - utilize more than one thread/core to avoid bottlenecks with sending requests
    let concurrency_levels = [4, 6, 8, 12, 16, 20, 24, 28, 32, 40, 48, 56, 64, 80, 96, 128];
    let max_concurrency = concurrency_levels.last().cloned().unwrap();
    let mut tasks = Vec::new();
    let (results_tx, results_rx) = flume::unbounded::<Option<std::num::NonZeroU32>>();
    let mut metrics = Vec::new();
    for concurrency in concurrency_levels {
        println!("Concurrency {}, warming up...", concurrency);
        // Gradually increase number of concurrent requests
        for tasl_idx in tasks.len()..concurrency {
            let client = client.clone();
            let url = url.clone();
            let requests = requests.clone();
            let results_tx = results_tx.clone();

            tasks.push(tokio::spawn(async move {
                // Avoid duplicate requests by cycling through the requests array
                let requests = requests
                    .iter()
                    .cycle()
                    .skip(tasl_idx)
                    .step_by(max_concurrency);
                for r in requests {
                    let start = std::time::Instant::now();
                    let result = client
                        .request(r.method.into(), &url)
                        .headers(
                            r.headers
                                .iter()
                                .map(|(k, v)| {
                                    (
                                        HeaderName::from_str(k).unwrap(),
                                        HeaderValue::from_str(v).unwrap(),
                                    )
                                })
                                .collect(),
                        )
                        .body(r.body.clone())
                        .send()
                        .await;
                    let elapsed_us = start.elapsed().as_micros();
                    let latency = if result.is_ok_and(|r| r.status().is_success()) {
                        Some(std::num::NonZeroU32::new(elapsed_us as u32).unwrap())
                    } else {
                        None
                    };

                    // Channel has been closed, stop sending results and exit the task
                    if results_tx.send(latency).is_err() {
                        break;
                    }
                }
            }));
        }

        // First 15s read all results and throw them away
        let start = std::time::Instant::now();
        while let Ok(_latency) = results_rx.recv() {
            if start.elapsed().as_secs() >= 15 {
                break;
            }
        }

        // Then read the results for 15s and count the success rate, throughput and p50, p95, p99 latency
        let start = std::time::Instant::now();
        let mut successfull = 0;
        let mut total = 0;
        let mut latencies = Vec::new();
        let mut metric = Metric::default();
        while let Ok(latency) = results_rx.recv() {
            total += 1;
            if let Some(latency) = latency {
                successfull += 1;
                latencies.push(latency.get());
            }
            let elapsed = start.elapsed();
            if elapsed.as_secs() >= 15 {
                metric.throughput = total as f64 / elapsed.as_secs_f64();
                metric.success_rate = successfull as f64 / total as f64;
                break;
            }
        }
        latencies.sort_unstable();
        metric.p50 = latencies[(latencies.len() as f64 * 0.50) as usize] as f64 / 1000.0;
        metric.p95 = latencies[(latencies.len() as f64 * 0.95) as usize] as f64 / 1000.0;
        metric.p99 = latencies[(latencies.len() as f64 * 0.99) as usize] as f64 / 1000.0;
        metrics.push(metric);
    }

    for m in metrics {
        println!(
            "Concurrency {}, throughput {:.2}rps, success rate {:.2}, p50 {:.1}ms, p95 {:.1}ms, p99 {:.1}ms",
            m.concurrency, m.throughput, m.success_rate, m.p50, m.p95, m.p99
        );
    }

    // Close the channel to stop all tasks
    drop(results_tx);
    drop(results_rx);
    for t in tasks {
        t.await.expect("Task panicked");
    }
}

/// Parses the TCP header and extracts the HTTP request payload if present.
fn get_tcp_data(data: &[u8]) -> Option<&[u8]> {
    if data.len() < 36 {
        return None;
    }

    let tcp_packet = if u16::from_be_bytes([data[14], data[15]]) == 0x0800 {
        // For `-i any` tcpdump adds 2 bytes of something to the Ethernet frame
        &data[16..]
    } else if u16::from_be_bytes([data[12], data[13]]) == 0x0800 {
        // In regular Ethernet frames the Ethernet header is 14 bytes long
        &data[14..]
    } else {
        println!("Not a Ethernet Type II frame start: {:02x?}.", &data[..16]);
        return None;
    };

    let ip_version_and_ihl = tcp_packet[0];
    let version = ip_version_and_ihl >> 4;
    if version != 4 {
        println!("Not an IPv4 packet (Version: {}).", version);
        return None;
    }
    let ip_protocol = tcp_packet[9];
    if ip_protocol != 6 {
        println!("Not a TCP packet (Protocol: {ip_protocol}).");
        return None;
    }

    let ihl = (ip_version_and_ihl & 0x0f) as usize * 4;
    let tcp_packet = &tcp_packet[ihl..];

    let data_offset = (tcp_packet[12] >> 4) as usize * 4;
    if tcp_packet.len() <= data_offset {
        return None;
    }
    Some(&tcp_packet[data_offset..])
}

fn parse_tcpdump(path: String) -> Result<Vec<Request>> {
    let file = File::open(path).context("Failed to open pcap file")?;
    let mut reader = LegacyPcapReader::new(65536, file).context("Failed to read pcap file")?;
    let mut requests = vec![];

    // Array of timestamps to calculate the requests per second
    let mut timestamps = vec![];

    loop {
        match reader.next() {
            Ok((offset, data)) => {
                if let PcapBlockOwned::Legacy(block) = data {
                    if let Some(tcp_data) = get_tcp_data(block.data) {
                        if let Ok(api) = proto::api::Api::parse_from_bytes(tcp_data) {
                            if let proto::options::options::Action::route =
                                api.options.action.enum_value_or_default()
                            {
                                // Convert the timestamp to microseconds
                                timestamps
                                    .push(block.ts_sec as u64 * 1_000_000 + block.ts_usec as u64);

                                // todo: actually, parse headers from the previous tcp packet
                                requests.push(Request {
                                    method: HttpMethod::GET,
                                    headers: [
                                        (header::CONTENT_TYPE, "application/x-protobuf"),
                                        (header::ACCEPT_ENCODING, "gzip, deflate"),
                                    ]
                                    .into_iter()
                                    .map(|(k, v)| (k.to_string(), v.to_string()))
                                    .collect(),
                                    body: api.write_to_bytes().unwrap(),
                                });

                                // if let Some(method) = tcp_data
                                //     .position(|b| b == b' ')
                                //     .and_then(|pos| Method::from_bytes(&tcp_data[..pos]).ok())
                                // {
                                //     // and then parse the headers and body
                                // }
                            }
                        }
                    }
                }
                reader.consume(offset);
            }
            Err(PcapError::Eof) => break,
            Err(PcapError::Incomplete(_)) => {
                reader.refill().unwrap();
            }
            Err(e) => panic!("error while reading: {:?}", e),
        }
    }

    let duration_s =
        (timestamps.last().unwrap_or(&0) - timestamps.first().unwrap_or(&0)) as f64 / 1_000_000.0;
    let peak_rps = count_peak_rps(timestamps);
    println!(
        "Parsed {} requests ({duration_s:.1}s), average {:.2}rps, peak {peak_rps}rps",
        requests.len(),
        requests.len() as f64 / duration_s,
    );

    Ok(requests)
}

fn count_peak_rps(mut timestamps: Vec<u64>) -> usize {
    timestamps.sort();

    let mut max_rps = 0;
    let mut start = 0;

    for end in 0..timestamps.len() {
        // Move the start pointer to maintain a 1-second window
        while timestamps[end] - timestamps[start] > 1_000_000 {
            start += 1;
        }

        // Calculate the number of requests in the current window
        let current_rps = end - start + 1;
        if current_rps > max_rps {
            max_rps = current_rps;
        }
    }

    max_rps
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn count_peak_rps_test() {
        assert_eq!(count_peak_rps(vec![]), 0);
    }

    #[test]
    fn get_tcp_data_test() {
        assert_eq!(get_tcp_data(&[]), None);
        assert_eq!(get_tcp_data(&[0; 36]), None);
    }

    #[test]
    fn save_load_playbook_test() {
        let playbook = Playbook {
            requests: vec![Request {
                method: HttpMethod::GET,
                headers: vec![("Content-Type".to_string(), "application/json".to_string())],
                body: vec![1, 2, 3],
            }],
        };
        let path = "/tmp/test.playbook";
        playbook.save(path).expect("Failed to save playbook");
        let loaded_playbook = Playbook::load(path).expect("Failed to load playbook");
        assert_eq!(playbook.requests.len(), loaded_playbook.requests.len());
        for (r1, r2) in playbook
            .requests
            .iter()
            .zip(loaded_playbook.requests.iter())
        {
            assert_eq!(r1.method, r2.method);
            assert_eq!(r1.headers, r2.headers);
            assert_eq!(r1.body, r2.body);
        }
    }
}
